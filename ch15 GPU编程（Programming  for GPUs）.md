在过去的几十年里，图形处理器（GPU）已经从仅能在屏幕上绘制图像的专用硬件设备，发展为能够执行复杂并行内核计算的通用计算设备。如今，几乎每台计算机都配备了传统CPU与GPU协同工作，许多程序通过将并行算法的部分计算任务从CPU传输到GPU上，实现了显著的性能加速。  

本章将阐述典型GPU的工作原理、GPU软硬件如何执行SYCL应用程序，以及在为GPU编写和优化并行内核时需要牢记的技巧与方法。  

## 性能注意事项 
与其他处理器类型一样，不同厂商的GPU乃至同厂商不同代次的产品都存在差异；因此，<font style="color:#DF2A3F;background-color:#FBF5CB;">针对某款设备的最佳实践未必适用于其他设备</font>。本章提供的建议可能对当前及未来的多种GPU均有裨益，但......  

>  为实现特定GPU的最佳性能，请务必参考GPU厂商提供的文档！  
>

 本章末尾提供了多家GPU厂商的技术文档链接。  

##  GPU的工作原理  
 本节阐述典型GPU的工作原理，并说明GPU与其他类型加速器的差异。  

###  GPU 构建模块（GPU Building Blocks）
 图15-1展示了一个高度简化的GPU架构，由三个高层级构建模块组成：  

1.  <font style="color:#DF2A3F;background-color:#FBF5CB;">执行资源</font>（ Execution resources  ）：<font style="color:#DF2A3F;">GPU的执行资源是指执行计算工作的处理器</font>。不同的GPU厂商对其执行资源采用不同的命名方式，但<font style="background-color:#E8F7CF;">所有现代GPU均由多个可编程处理器构成</font>。这些处理器可能是异构的，专用于特定任务（如顶点变换和像素着色），也可能是同构且可互换的。大多数现代GPU的处理器属于同构可互换类型。  
2.  <font style="color:#DF2A3F;background-color:#FBF5CB;">固定功能单元</font>（Fixed functions）：<font style="color:#DF2A3F;">GPU固定功能单元是比执行资源可编程性更低的硬件模块，专用于单一任务</font>。当GPU用于图形处理时，渲染管线中的诸多环节（如光栅化或光线追踪）通过固定功能单元实现，以提升能效与性能。<font style="background-color:#E8F7CF;">当GPU用于数据并行计算时，固定功能单元可执行工作负载调度、纹理采样及依赖关系追踪等任务</font>。  
3.  <font style="color:#DF2A3F;background-color:#FBF5CB;">缓存与内存</font>（ Caches and memory  ）：与其他处理器类型类似，<font style="color:#DF2A3F;">GPU通常配备缓存以存储执行单元访问的数据</font>。GPU缓存可分为隐式缓存（程序员无需主动操作）和显式暂存存储器（程序员必须在使用前手动将数据移入缓存）。许多GPU还拥有大容量内存池，为执行单元提供快速数据访问支持。  

![图15-1. 典型GPU构建模块——非实际比例！](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743667636677-7ad3494f-7494-4609-afea-35c26fb96126.png)

###  更简单的处理器（但数量更多）  
传统上，在执行图形运算时，GPU会处理大批量数据。例如，典型的游戏帧或渲染工作负载涉及数千个顶点，每帧生成数百万像素。为了维持交互式帧率，这些海量数据必须被尽可能快速地处理。  

典型的GPU设计权衡策略是：从构成执行资源的处理器中剔除那些用于加速单线程性能的功能模块，并将由此节省的资源用于构建更多处理器（如图15-2所示）。例如，<font style="background-color:#E8F7CF;">GPU处理器通常不会配备其他类型处理器所采用的复杂乱序执行能力或分支预测逻辑</font>。由于这种设计取舍，<font style="background-color:#E8F7CF;">单个数据元素在GPU上的处理速度可能慢于其他处理器，但更大规模的处理器阵列使得GPU能够快速高效地并行处理海量数据元素</font>。  

![图15-2. GPU处理器更简单，但数量更多](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743667756790-2dfec796-921c-4f47-b7b2-db6172bd6b6e.png)

为了在执行内核时充分利用这种权衡，关键在于<font style="color:#DF2A3F;">为GPU提供足够大范围的数据元素进行处理</font>。为了说明传输大量数据的重要性，请考虑本书中我们持续开发和修改的矩阵乘法内核。  

> ** 关于矩阵乘法的提醒  **
>
> 本书以矩阵乘法核为例，阐释了内核或其调度方式的变更如何影响性能。尽管通过本章所述技术可显著提升矩阵乘法运算效率，但鉴于该运算在各类硬件（GPU、CPU、FPGA、DSP等）中的基础性与普适性，众多硬件供应商已针对包括矩阵乘法在内的多种例程开发了高度优化的实现方案。这些供应商投入大量时间精力为特定设备定制并验证函数，有时甚至会采用标准内核难以实现或无法复现的特殊功能与技术。  
>
>  **使用供应商提供的库！  **
>
> <font style="color:#DF2A3F;">当厂商提供某个函数的库实现时，几乎总是优先使用该实现而非在核心代码中重新实现</font>！oneMKL项目（作为oneAPI的组成部分）提出了能根据硬件平台智能调用计算库的接口方案——针对英特尔平台调用英特尔MKL，英伟达平台调用cuBLAS，AMD平台调用HIPBLAS。若此类接口方案可用，将大幅简化开发流程。反之，则需自行确保为目标硬件选用最优计算库。  
>

矩阵乘法内核可以通过将其作为单一任务提交到队列中，在GPU上轻松执行。该矩阵乘法内核的主体与在主机CPU上执行的函数完全一致，如图15-3所示。  

```cpp
// 图15-3 单任务矩阵乘法运算的实现与CPU主机端代码高度相似
h.single_task([i=]() {
    for (int m = 0; m < M; m++) {
        for (int n = 0; n < N; n++) {
            T sum = 0;
            for (int k = 0; k < K; k++) {
                sum +=
                    matrixA[m * K + k] * matrixB[k * N + n];
            }
            matrixC[m * N + n] = sum;
        }
    }
});
```

 若我们尝试在CPU上执行此内核，其表现或许尚可——虽不突出，因未期望利用CPU的任何并行计算能力，但对于小规模矩阵可能已足够。然而如图15-4所示，若改在GPU上执行该内核，则性能很可能极其低劣，因为单一任务仅能调用单个GPU处理器。  

![图15-4. 单任务GPU内核会导致大量执行资源闲置](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743668109511-6c886328-e6d7-4f55-9956-2ff2540b6553.png)

#### 表达并行性（ Expressing Parallelism  ）
为提升该内核在CPU和GPU上的性能，我们可以通过<font style="background-color:#E8F7CF;">将其中一个循环转换为parallel_for，提交待并行处理的数据元素范围</font>。对于矩阵乘法内核，可选择提交代表两个最外层循环中任意一个的数据元素范围。如图15-5所示，我们已选择并行处理结果矩阵的行数据。  

```cpp
// 图15-5. 半并行（ Somewhat-parallel）矩阵乘法
h.parallel_for(range{M}, [=](id<1> idx) {
    int m = idx[0];

    for (int n = 0; n < N; n++) {
        T sum = 0;
        for (int k = 0; k < K; k++) {
            sum += matrixA[m * K + k] * matrixB[k * N + n];
        }
        matrixC[m * N + n] = sum;
    }
});
```

> ** 选择并行化方式 ** 
>
>  选择在哪个维度进行并行化是优化应用程序（无论是针对GPU还是其他设备类型）的一种重要方式。本章后续章节将阐述为何在某些维度进行并行化可能比其他维度获得更优性能。  
>

尽管半并行（ Somewhat-parallel  ）内核与单任务内核极为相似，但其在CPU上的运行效率应更优，在GPU上的性能提升则更为显著。如图15-6所示，parallel_for机制使代表结果矩阵各行的工作项能够在多个处理器资源上并行处理，从而确保所有计算资源持续处于高效工作状态。  

![图15-6. 半并行（Somewhat-parallel）内核能更充分地利用处理器资源](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743668388488-28d2c278-7ee4-4d10-96fc-13f6ae71d37d.png)

需要注意的是，具体如何将行分区并分配给不同的处理器资源并未明确规定，这为实际实现提供了灵活性，可根据设备特性选择最优的内核执行方式。例如，实现方案可选择在同一处理器上连续执行相邻行而非单行处理，从而获得局部性优势。  

#### 表达更多并行性（Expressing More Parallelism）
我们可以通过选择并行处理两个外层循环来进一步并行化矩阵乘法核函数。由于parallel_for能够表达多达三维的并行循环，如图15-7所示，这种实现非常直观。在图15-7中需注意，传递给parallel_for的范围参数和代表并行执行空间索引的item对象现在都是二维的。  

```cpp
// 图15-7. 更高阶的并行矩阵乘法
h.parallel_for(range{M, N}, [=](id<2> idx) {
    int m = idx[0];
    int n = idx[1];

    T sum = 0;
    for (int k = 0; k < K; k++) {
        sum += matrixA[m * K + k] * matrixB[k * N + n];
    }

    matrixC[m * N + n] = sum;
});
```

<font style="color:#DF2A3F;">暴露更多的并行性可能会提升矩阵乘法核在GPU上运行时的性能</font>。<font style="background-color:#E8F7CF;">即便矩阵行数超过GPU处理器数量，这一结论仍可能成立</font>。后续几节将探讨可能的原因。  

###  简化控制逻辑（SIMD指令）  
<font style="background-color:#E8F7CF;">许多GPU处理器通过利用多数数据元素倾向于遵循相同控制流路径通过内核这一特性来优化控制逻辑</font>。例如，在矩阵乘法内核中，由于循环边界不变，每个数据元素执行最内层循环的次数相同。  

当数据元素在核内采取相同的控制流路径时，处理器可通过在多个数据元素间共享控制逻辑并将其分组执行，从而降低指令流管理开销。实现方式之一是采用<font style="color:#DF2A3F;background-color:#FBF5CB;">单指令多数据（SIMD）指令集架构</font>，即<font style="color:#DF2A3F;background-color:#FBF5CB;">通过单条指令同时处理多个数据元素</font>。  

>  **线程与指令流的比较  **
>
> 在许多并行编程环境和GPU文献中，"thread"（线程）一词被用来指代"instruction stream"（指令流）。在这些语境下，它不同于传统操作系统中的线程概念，通常具有更轻量级的特性。但这种情况并非绝对，某些场合下"thread"可能被用来描述完全不同的概念。  
>
>  由于“thread（线程）”一词存在多重含义且易被误解（即便在不同GPU厂商之间亦然），本章将采用“instruction stream（指令流）”这一术语替代。  
>

![图15-8. 四路并行SIMD处理器：四个算术逻辑单元共享取指/译码逻辑](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743668849094-7d8a9c39-f7bd-46bb-88e5-f4c966cc18a0.png)

由单条指令同时处理的数据元素数量，有时被称为该指令或执行该指令的处理器的<font style="color:#DF2A3F;background-color:#FBF5CB;">SIMD宽度</font>。在图15-8中，四个算术逻辑单元共享相同的控制逻辑，因此可将其描述为四路并行SIMD处理器。  

GPU处理器并非唯一实现SIMD指令集的处理器类型。其他类型处理器同样采用SIMD指令集以提高大数据集处理效率。<font style="color:#DF2A3F;background-color:#FBF5CB;">GPU与其他处理器的核心差异在于</font>：<font style="background-color:#E8F7CF;">GPU依赖并行执行多个数据元素以获得优异性能，且通常支持更宽的SIMD位宽</font>。例如，GPU处理器支持16、32甚至更多数据元素的SIMD位宽实属常见。  

>  **编程模型：单程序多数据（SPMD）与单指令多数据（SIMD）  **
>
> 尽管GPU处理器实现了不同宽度的SIMD指令集，但这通常属于实现细节，对于在GPU上执行数据并行内核的应用程序是透明的。这是因为许多GPU编译器和运行时API采用单程序多数据（SPMD）编程模型，<font style="color:#DF2A3F;background-color:#FBF5CB;">由编译器和运行时API自动确定用SIMD指令流处理哪些数据元素组效率最高，而非显式表达SIMD指令</font>。第9章"子工作组"一节将探讨数据元素分组对应用程序可见的特殊情况。  
>

在图15-9中，我们将每个执行单元的宽度扩展为四路SIMD架构，从而能够并行处理的矩阵行数提升至四倍。  

![图15-9. 在SIMD处理器上执行半并行（Somewhat-parallel）内核](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743669027221-a13b2ca0-8dbf-4892-9d3a-69e8a10c6094.png)

利用SIMD指令并行处理多个数据元素，是实现图15-5和图15-7中并行矩阵乘法内核性能超越单纯处理器数量限制的途径之一。SIMD指令的使用还能通过在同一处理器上执行连续数据元素的操作，为包括矩阵乘法在内的诸多场景带来天然的局部性优势。  

<font style="color:#DF2A3F;background-color:#FBF5CB;"> 内核受益于处理器间的并行性与处理器内的并行性！</font>  

####  断言与掩码（Predication andMasking）
在多数据元素间共享指令流的效果良好，只要所有数据元素在内核的条件代码中遵循相同执行路径。当数据元素在条件代码中采取不同路径时，即称为控制流出现分支。在SIMD指令流中出现控制流分支时，通常会同时执行两条控制路径，同时通过通道掩码或谓词执行机制关闭部分通道。这种做法虽能确保行为正确性，但会带来性能代价——<font style="color:#DF2A3F;">被掩码的通道无法执行有效计算工作</font>。  

为说明谓词执行与掩码操作的原理，以图15-10中的内核为例：该内核将所有"奇数"索引的数据元素乘以二，并将所有"偶数"索引的数据元素加一。  

```cpp
// 图15-10. 具有发散控制流的内核
h.parallel_for(array_size, [=](id<1> i) {
    auto condition = i[0] & 1;
    if (condition) {
        dataAcc[i] = dataAcc[i] * 2;  // odd
    } else {
        dataAcc[i] = dataAcc[i] + 1;  // even
    }
});
```

假设我们在图15-8所示的四通道SIMD处理器上执行该内核，前四个数据元素在一个SIMD指令流中执行，随后四个数据元素在另一个SIMD指令流中执行，以此类推。图15-11展示了通过掩码通道和谓词执行实现控制流分叉时正确运行该内核的其中一种方式。  

![图15-11 发散核的可能通道掩码](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743669388834-6f7161b9-e45e-4f63-8a02-6dbc59e83241.png)

####  SIMD效率 
<font style="color:#DF2A3F;">SIMD效率衡量的是SIMD指令流相较于等效标量指令流的性能表现</font>。如图15-11所示，由于控制流将通道划分为两个等规模组，发散控制流中的每条指令仅以半数效率执行。在最坏情况下，对于高度发散的内核，效率可能会降低至处理器SIMD宽度的倒数倍。  

<font style="color:#DF2A3F;">所有实现SIMD指令集的处理器都会受到影响SIMD效率的分支惩罚（divergence penalties）困扰</font>。但由于GPU处理器通常比其他类型处理器支持更宽的SIMD宽度，<font style="background-color:#E8F7CF;">在针对GPU优化内核时，重构算法以最小化分支控制流并最大化聚合执行（converged execution）可能特别有益</font>。虽然这并非总能实现，但举例而言：选择在聚合执行程度更高的维度进行并行化，其性能可能优于在分支高度分散（highly divergent）的其他维度开展并行化。  

####  SIMD效率与项组（SIMD Efficiency and Groups of Items）
本章至此讨论的所有核函数均为基础数据并行核函数，其执行范围内未指定工作项的显式分组，这为实现层提供了根据设备特性选择最优分组的自由度。例如，支持更宽SIMD位宽的设备可能倾向采用更大的分组规模，而SIMD位宽较窄的设备采用较小分组即可获得良好性能。  

<font style="color:#DF2A3F;">当内核是具有显式工作项分组的ND范围内核时，应注意选择能最大化SIMD效率的ND范围工作组规模</font>。<font style="background-color:#E8F7CF;">若工作组规模无法被处理器的SIMD宽度整除，部分工作组可能在核函数整个执行期间以禁用通道的方式运行</font>。<font style="color:#DF2A3F;background-color:#FBF5CB;">可通过设备特定的内核查询preferred_work_group_size_multiple来选取高效的工作组规模</font>。有关如何查询设备属性的更多信息，请参阅第12章。  

选择仅包含单个工作项的工作组规模很可能导致性能极差，因为许多GPU会通过屏蔽除一个通道外的所有SIMD通道来实现单工作项工作组。例如，图15-12所示内核的性能很可能远逊于图15-5中高度相似的内核——尽管两者间唯一显著差异仅是从基础数据并行内核转变为低效的单工作项ND范围内核（nd_range<1>{M, 1}）。  

```cpp
// 图15-12. 低效的单条目半并行矩阵乘法
h.parallel_for(
    nd_range<1>{M, 1}, [=](nd_item<1> idx) {
        int m = idx.get_global_id(0);

        for (int n = 0; n < N; n++) {
            T sum = 0;
            for (int k = 0; k < K; k++) {
                sum += matrixA[m * K + k] * matrixB[k * N + n];
            }
            matrixC[m * N + n] = sum;
        }
    }
);
```

### 切换工作以隐藏延迟
许多GPU还采用了一项技术来简化控制逻辑、最大化执行资源并提升性能：它们并非在单个处理器上执行单一指令流，而是<font style="color:#DF2A3F;background-color:#E8F7CF;">允许多个指令流同时驻留在同一处理器上</font>。  

在处理器上驻留多个指令流是有益的，因为它为每个处理器提供了可执行工作的选择。如果一个指令流正在执行长延迟操作（例如从内存读取数据），处理器可以切换到另一个准备就绪的指令流，而无需等待当前操作完成。当指令流足够多时，处理器切换回原始指令流时，长延迟操作可能已经完成，完全不需要处理器等待。  

图15-13展示了处理器如何利用多指令流并行执行来隐藏延迟并提升性能。虽然首个指令流在多重指令流模式下执行耗时略有增加，但通过切换至其他指令流，处理器能够持续执行就绪任务，完全无需空转等待长延时操作的完成。  

![图15-13. 切换指令流以隐藏延迟](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743669790724-45494ca3-d64c-479d-abf0-0cdf572721b4.png)

GPU分析工具在描述GPU处理器当前执行的指令流数量与理论最大指令流数量的比值时，可能会使用"占用率(occupancy)"这一术语。  

<font style="color:#DF2A3F;">低占用率并不必然意味着低性能</font>，因为少量指令流也可能使处理器保持忙碌状态。同理，<font style="color:#DF2A3F;">高占用率也不必然代表高性能</font>，因为若所有指令流均执行低效且高延迟的操作，GPU处理器仍可能处于等待状态。然而在其他条件相同的情况下，提高占用率能最大化GPU处理器的延迟隐藏能力，通常也会提升性能。增加占用率是图15-7中并行度更高的内核可能带来性能提升的另一原因。  

这种<font style="color:#DF2A3F;background-color:#FBF5CB;">通过在多条指令流之间切换以隐藏延迟的技术特别适用于GPU和数据并行处理</font>。回顾图15-2可知，GPU处理器通常比其他类型的处理器更简单，因此缺乏复杂的延迟隐藏功能。这使得GPU处理器更容易受到延迟问题的影响，但由于数据并行编程涉及大量数据处理，GPU处理器通常拥有充足的指令流可供执行！  

## 将内核传输到GPU
本节阐述了应用程序、SYCL运行时库与GPU软件驱动程序如何协同工作，将内核传输至GPU硬件执行。图15-14中的示意图展示了包含这些抽象层的典型软件栈结构。多数情况下，应用程序无需感知这些层次的存在，但在调试或性能分析时，理解并考量这些层级至关重要。  

![图15-14. 将并行内核传输至GPU（简化示意图）](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743669987492-9d1d694d-c95e-4077-b5e8-9485a8daaf4f.png)

### SYCL运行时库（SYCL Runtime Library）
SYCL运行时库是SYCL应用程序交互的核心软件库。该运行时库负责实现队列(queue)、缓冲区(buffer)和访问器(accessor)等类及其成员函数。部分运行时库可能以头文件形式存在，因而会被直接编译到应用程序可执行文件中；其余部分则作为库函数实现，在应用程序构建过程中与可执行文件链接。该运行时库通常不针对特定设备，同一运行时库可协调向CPU、GPU、FPGA或其他设备的计算传输。

### GPU 软件驱动程序（GPU Software Drivers）
虽然从理论上讲，SYCL运行时库有可能直接传输到GPU上，但在实践中，大多数SYCL运行时库会通过与GPU软件驱动交互来向GPU提交任务。  

<font style="color:#DF2A3F;">GPU软件驱动程序通常是某种API（如OpenCL、Level Zero或CUDA）的具体实现</font>。其大部分功能通过用户模式驱动库实现，SYCL运行时通过调用该库进行交互；用户模式驱动则可能进一步调用操作系统或内核模式驱动，以执行系统级任务（如内存分配或向设备提交工作任务）。<font style="color:#DF2A3F;">该驱动还可能调用其他用户模式库</font>——例如，GPU驱动程序可即时调用GPU编译器，将内核代码从中间表示形式动态编译为GPU指令集架构（ISA）。图15-15展示了这些软件模块及其交互关系。  

![图15-15. 典型GPU软件驱动模块](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743670245389-9f7c0cc5-1fc5-42ab-8ab2-7c86e1370b0f.png)

### GPU硬件
当运行时库或GPU软件用户模式驱动被显式请求提交任务时，亦或当GPU软件通过启发式判定应当启动任务时，通常将通过操作系统或内核模式驱动调用来启动GPU上的任务执行。某些情况下，GPU软件用户模式驱动可能直接将任务提交至GPU，但此类情况较为罕见，且并非所有设备或操作系统均支持此操作。

<font style="color:#DF2A3F;">当GPU上执行的工作结果被主机处理器或其他加速器使用时，GPU必须发出信号以表明工作已完成</font>。工作完成的步骤与工作提交的步骤非常相似，但顺序相反：GPU会向操作系统或内核模式驱动程序发出执行完毕的信号，随后用户模式驱动程序将收到通知，最终运行时库通过GPU软件API调用检测到工作已完成。  

<font style="color:#DF2A3F;background-color:#FBF5CB;">这些步骤中的每一步都会引入延迟</font>，而在许多情况下，<font style="background-color:#E8F7CF;">运行时库和GPU软件需要在降低延迟与提高吞吐量之间进行权衡</font>。例如，更频繁地向GPU提交任务可能会减少延迟，但由于每次提交的开销，频繁提交也可能降低吞吐量。相反，收集大批量任务会增加延迟，但能将提交开销分摊到更多任务中，同时创造更多并行执行的机会。运行时系统和驱动程序经过调优以做出恰当的权衡，通常表现良好——但若怀疑驱动程序的启发式算法存在低效提交行为，则应查阅文档，确认是否存在通过API特定甚至实现特定机制来覆盖默认驱动程序行为的方法。第20章所述<font style="background-color:#E8F7CF;">直接与API后端交互的技术，可用于优化GPU任务提交策略</font>。  

### 警惕传输成本！  
尽管SYCL实现和GPU供应商持续创新优化以降低向GPU传输工作的成本，但无论是在GPU上启动工作还是在主机或其他设备上观察结果，总会存在一定的开销。<font style="color:#DF2A3F;">在选择算法执行位置时，需同时权衡设备执行算法带来的优势与迁移算法及其所需数据至设备的成本</font>。某些情况下，为避免算法在不同处理器间迁移的开销，使用主机处理器执行并行操作——或低效地在GPU上运行算法的串行部分——可能反而最具效率。  

> 试想我们算法的整体性能——有时将算法的某部分低效地在一台设备上执行，可能比将其转移到另一台设备更为高效！  
>

####  与设备内存之间的数据传输（Transfers to and from Device Memory）
<font style="background-color:#E8F7CF;">在配备专用内存的GPU上，需特别注意专用GPU内存与主机或其他设备内存之间的传输开销</font>。图15-16展示了系统中不同类型内存间典型的带宽差异。  

![图15-16. 设备内存、远程内存与主机内存的典型差异](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743670555077-266db746-1376-4f72-afc5-5c37eb214816.png)

回顾第3章内容可知，<font style="color:#DF2A3F;">GPU更倾向于在专用设备内存上运行，其速度可比主机内存或其他设备内存快一个数量级甚至更多</font>。尽管访问专用设备内存的速度远高于访问远程内存或系统内存，但<font style="background-color:#FBF5CB;">若数据尚未存储在专用设备内存中，则必须进行复制或迁移操作</font>。  

<font style="color:#DF2A3F;background-color:#FBF5CB;">只要数据会被频繁访问，将其移至专用设备内存便是有益的</font>，尤其是当数据传输能在GPU执行资源忙于处理其他任务时异步完成的情况下。然而，当数据访问频率较低或难以预测时，则更适合节省传输成本，直接在远程或系统内存中操作数据——即使单次访问成本更高。第6章将阐述控制内存分配位置的方法，以及将数据复制和预取至专用设备内存的不同技术。这些技术对于优化GPU程序执行至关重要。  

## GPU内核最佳实践（GPU Kernel Best Practices）
前文章节阐述了传递给`parallel_for`的调度参数如何影响内核向GPU处理器资源的分配，以及GPU上执行内核所涉及的软件层级与开销。本节将探讨内核在GPU上执行时的最佳实践方案。  

广义而言，<font style="color:#DF2A3F;">内核可分为</font><font style="color:#DF2A3F;background-color:#FBF5CB;">内存受限型</font><font style="color:#DF2A3F;">与</font><font style="color:#DF2A3F;background-color:#FBF5CB;">计算受限型</font><font style="color:#DF2A3F;">两类</font>。<font style="color:#DF2A3F;background-color:#FBF5CB;">内存受限型内核</font>的<font style="background-color:#E8F7CF;">性能受限于GPU执行单元的数据读写操作</font>，而<font style="color:#DF2A3F;background-color:#E8F7CF;">计算受限型内核</font>的性能则<font style="background-color:#E8F7CF;">受限于GPU执行单元本身的计算能力</font>。在为GPU（及多数其他处理器）优化内核时，首要步骤是判定内核属于内存受限型还是计算受限型——<font style="background-color:#E8F7CF;">因为提升内存受限型内核的技术往往对计算受限型内核无效，反之亦然</font>。GPU厂商通常会提供性能分析工具以辅助此类判断。  

>  **<font style="color:#DF2A3F;">根据内核是受内存限制还是受计算限制，需要采用不同的优化技术！  </font>**
>

由于GPU通常拥有大量处理器和宽SIMD位宽，其内核往往更容易受限于内存带宽而非计算能力。若不确定优化方向，**<font style="color:#DF2A3F;">首先分析内核的内存访问模式是明智之举</font>**。  

###  访问全局内存(Accessing Global Memory)
<font style="color:#DF2A3F;">高效访问全局内存对应用程序性能优化至关重要</font>，因为<font style="background-color:#E8F7CF;">工作项（work-item）或工作组（work-group）处理的数据几乎都源自全局内存</font>。若内核（kernel）对全局内存的访问效率低下，其性能必然表现不佳。尽管GPU通常配备专用硬件聚集-分散单元（gather-scatter unit）用于读写内存中的任意位置，但<font style="color:#DF2A3F;background-color:#FBF5CB;">全局内存访问性能主要取决于数据访问的局部性</font>。<font style="background-color:#E8F7CF;">当</font><font style="color:#DF2A3F;background-color:#E8F7CF;">工作组</font><font style="background-color:#E8F7CF;">中某个</font><font style="color:#DF2A3F;background-color:#E8F7CF;">工作项</font><font style="background-color:#E8F7CF;">访问的内存元素与</font><font style="color:#DF2A3F;background-color:#E8F7CF;">组内其他工作项</font><font style="background-color:#E8F7CF;">访问的元素相邻时，</font><font style="color:#DF2A3F;background-color:#E8F7CF;">全局内存访问性能往往较优</font>；反之，<font style="background-color:#E8F7CF;">若工作组内工作项访问的内存呈跨步（strided）或随机分布，则性能通常会下降</font>。部分GPU文档将这种邻近内存访问操作描述为<font style="color:#DF2A3F;background-color:#FBF5CB;">合并内存访问</font>（coalesced memory accesses）。  

请记住，在图15-5展示的准并行矩阵乘法核函数中，我们曾面临选择以并行方式处理结果矩阵的行还是列，最终选择了并行处理结果矩阵的行。这一选择被证明是不明智的：<font style="color:#DF2A3F;background-color:#FBF5CB;">若ID为m的工作项与相邻ID为m-1或m+1的工作项被分组，各工作项访问matrixB的索引相同，但访问matrixA的索引相差K值，这意味着访问跨度极大</font>。图15-17展示了matrixA的访问模式。  

![图15-17。对矩阵A的访问步幅过大且效率低下。](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743678404662-33ab1dee-4f46-4a30-baeb-03202befaea7.png)

若我们转而选择并行处理结果矩阵的列，其访问模式将具有更优的局部性。图15-18所示内核在结构上与图15-5高度相似，唯一区别在于：图15-18中的每个工作项操作的是结果矩阵的列向量，而非结果矩阵的行向量。  

```cpp
// 图15-18. 并行计算结果矩阵的列而非行
h.parallel_for(N, [=](item<1> idx) {
    int n = idx[0];

    for (int m = 0; m < M; m++) {
        T sum = 0;
        for (int k = 0; k < K; k++) {
            sum += matrixA[m * K + k] * matrixB[k * N + n];
        }
        matrixC[m * N + n] = sum;
    }
});
```

尽管两个内核在结构上非常相似，<font style="color:#DF2A3F;background-color:#FBF5CB;">但在许多GPU上，</font><font style="color:#DF2A3F;background-color:#E8F7CF;">对数据列进行操作</font><font style="color:#DF2A3F;background-color:#FBF5CB;">的内核性能会显著优于对</font><font style="color:#DF2A3F;background-color:#E8F7CF;">数据行进行操作</font><font style="color:#DF2A3F;background-color:#FBF5CB;">的内核</font>，这完全得益于更高效的内存访问：<font style="color:#DF2A3F;">若将编号为n的工作项与相邻编号为n-1或n+1的工作项分组，则每个工作项访问矩阵A的索引将保持一致，而访问矩阵B的索引则呈现连续状态</font>。图15-19展示了矩阵B的访问模式。  

![图15-19. 对矩阵B的访问是连续且高效的。](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743678501456-f1b5030b-413e-4c01-82c0-486ffc23164a.png)

对连续数据的访问通常非常高效。一个实用的经验法则是：<font style="color:#DF2A3F;background-color:#FBF5CB;">工作组内工作项对全局内存的访问性能，与所访问的GPU缓存行数量呈函数关系</font>。<font style="background-color:#E8F7CF;">若所有访问均位于单个缓存行内，则能以峰值性能执行</font>；若访问需要跨越两个缓存行（例如隔元素访问或从非对齐地址起始），性能可能降至峰值的一半；当组内每个工作项访问独立的缓存行（如大跨度或随机访问时），性能很可能降至最低水平。  

> ** 分析内核变体 ** 
>
> 在矩阵乘法运算中，沿某一维度进行并行化显然能实现更高效的内存访问；但对于其他计算核心而言，最优并行维度的选择可能并不明确。针对那些对性能要求极高的计算核心，若难以直观判断最佳并行维度，有时值得开发并分析不同维度的并行化内核变体，通过实测确定何种方案更适配特定硬件设备与数据集。  
>

###  访问工作组本地内存（Accessing Work-Group Local Memory）
在上一节中，我们阐述了全局内存访问如何通过利用局部性来提升缓存性能。正如所见，在某些情况下可以通过算法设计实现高效内存访问（例如选择沿某一维度而非另一维度进行并行化）。但该技术并非适用于所有场景。本节将探讨<font style="color:#DF2A3F;background-color:#FBF5CB;">如何利用工作组局部内存，从而高效支持更广泛的内存访问模式</font>。  

回望第9章所述，<font style="color:#DF2A3F;background-color:#E8F7CF;">工作组中的工作项可通过工作组本地内存进行通信，并使用工作组屏障同步，以此协作解决问题</font>。该技术对GPU尤为有利，因为<font style="background-color:#E8F7CF;">典型GPU配备专用硬件来实现屏障和工作组本地内存</font>。不同GPU厂商及产品对工作组本地内存的实现方式或有差异，但与全局内存相比，其通常具备两大优势：<font style="color:#DF2A3F;">即便全局内存访问命中缓存，</font><font style="color:#DF2A3F;background-color:#E8F7CF;">本地内存仍能提供更高带宽和更低延迟</font>；此外，本地内存常被划分为称为"存储体"的不同区域。<font style="color:#DF2A3F;background-color:#E8F7CF;">只要组内每个工作项访问不同存储体，本地内存访问便能以全性能执行</font>。这种分体式访问使本地内存能以峰值性能支持比全局内存更丰富的访问模式。  

许多GPU厂商会将连续的本地内存地址分配给不同的存储体（bank）。这种设计确保了连续的存储器访问始终能以最高性能运行，而无需考虑起始地址。然而当存储器访问呈现跨步模式时，工作组内的某些工作项可能会访问分配到同一存储体的内存地址。这种情况称为<font style="color:#DF2A3F;background-color:#FBF5CB;">存储体冲突</font>（bank conflict），将导致访问序列化并降低性能。  

> **<font style="color:#DF2A3F;">为获得最佳全局内存性能，应尽量减少访问的缓存行数量。   
</font>****<font style="color:#DF2A3F;">为获得最佳局部内存性能，应尽量减少存储体冲突次数！</font>**  
>

图15-20展示了全局内存与局部内存的访问模式摘要及其预期性能表现。假设当指针ptr指向全局内存时，该指针对齐GPU缓存行的大小。访问全局内存时，最佳性能可通过从缓存行对齐地址开始连续访问来实现。访问非对齐地址可能会降低全局内存性能，因为此类访问可能需要涉及额外的缓存行。由于访问非对齐局部地址不会导致额外的存储体冲突，因此局部内存性能不受影响。  

跨步访问（strided）的情况值得更详细说明。访问全局内存中每隔一个元素需要读取更多缓存行，很可能导致性能下降。访问局部内存中每隔一个元素可能引发存储体冲突并降低性能，但仅当存储体数量为偶数时才会发生。若存储体数量为奇数，此情况仍可保持完整性能。  

<font style="color:#DF2A3F;background-color:#FBF5CB;">当访存步长（stride）很大时</font><font style="color:#DF2A3F;">，每个工作项（work-item）会访问独立的缓存行，导致最差性能表现</font>。但对于局部存储器而言，性能取决于步长与存储体（bank）数量的关系：当步长N等于存储体数量时，每次访问都会引发<font style="color:#DF2A3F;background-color:#FBF5CB;">存储体冲突</font>（bank conflict），所有访问操作被迫串行化，从而产生最差性能；若步长M与存储体数量互质，则能实现全速访问。因此，许多优化后的GPU内核会对局部存储器中的数据结构进行填充（padding），通过调整步长来减少或消除存储体冲突。  

![图15-20. 全局内存与本地内存在不同访问模式下的潜在性能表现](https://cdn.nlark.com/yuque/0/2025/png/33636091/1743678830212-9ced59c5-6c30-4d9b-a9cb-79d9ca5f9b96.png)

### 通过子群组实现完全规避本地内存 
如第9章所述，子组集体函数是工作组内工作项之间交换数据的另一种方式。<font style="color:#DF2A3F;">对于许多GPU而言，子组代表由单一指令流处理的工作项集合</font>。在这种情况下，子组中的工作项无需使用工作组本地内存，即可低成本地交换数据并实现同步。众多高性能GPU内核均采用子组机制，因此对于计算密集型内核，我们非常值得探究算法是否可重构为利用子组集体函数实现。  

### 采用小数据类型的计算优化
本节阐述了在消除或减少内存访问瓶颈后优化内核的技术。需要牢记的一个关键视角是：GPU最初是为屏幕绘图设计的。尽管GPU的纯计算能力已随时间不断演进和提升，但其图形处理的基因在某些领域仍清晰可见。  

以对内核数据类型的支持为例。许多GPU针对32位浮点运算进行了高度优化，因为这些操作在图形和游戏应用中十分常见。对于可适应较低精度的算法，许多GPU还支持以精度换取速度的16位低精度浮点类型。反之，尽管许多GPU确实支持64位双精度浮点运算，但额外的精度会带来性能代价，32位运算的表现通常远优于64位运算。 

 对于整数数据类型同样如此：32位整型通常比64位整型性能更优，而16位整型可能表现更佳。<font style="color:#DF2A3F;background-color:#FBF5CB;">若能通过计算结构调整使用更小的整数类型，内核性能可能进一步提升</font>。需特别注意寻址操作——这类操作通常基于64位的size_t数据类型，但有时可通过重构计算流程使主要运算采用32位数据类型完成。在某些局部内存场景中，由于分配空间较小，16位索引位宽已完全够用。  

### 优化数学函数  
内核在精度与性能之间进行权衡的另一个领域涉及SYCL内置函数。SYCL包含丰富的数学函数集，这些函数在特定输入范围内具有明确定义的精度。大多数GPU并不原生支持这些函数，而是通过一长串其他指令来实现它们。尽管这些数学函数的实现通常针对GPU进行了良好优化，但如果我们的应用能够容忍较低精度，就应考虑采用精度更低但性能更高的替代实现方案。有关SYCL内置函数的更多信息，请参阅第18章。  

对于常用的数学函数，SYCL库提供了快速（fast）或原生（native）函数变体，这些变体的精度要求有所降低或由具体实现定义。在某些GPU上，这些函数的执行速度可比其精确版本快一个数量级，因此只要算法对精度要求不高，就非常值得考虑使用。例如，许多图像后处理算法具有明确的输入范围，并能容忍较低精度，因此是使用快速或原生数学函数的理想场景。  

>  **<font style="color:#DF2A3F;">若算法能容忍较低精度，我们便可采用更小的数据类型或低精度数学函数以提升性能！  </font>**
>

### 专业功能与扩展（Specialized Functions and Extensions）
在针对GPU优化内核时，最后需要考虑的是许多GPU常见的专用指令。例如，几乎所有GPU都支持单周期执行两次运算的乘加指令（mad或fma）。GPU编译器通常能出色地将单独的乘法和加法优化为单条指令，但SYCL也提供了可显式调用的mad和fma函数。当然，若期望GPU编译器自动优化乘加运算，必须确保不会因禁用浮点收缩而阻碍优化！  

其他专用GPU指令可能仅能通过编译器优化、SYCL语言扩展或直接与底层GPU后端交互来实现。例如，某些GPU支持专用的点积累加指令，编译器会尝试识别并优化该指令，或可直接调用该指令。有关如何查询GPU实现所支持扩展的更多信息，请参阅第12章；关于后端互操作性的信息，请参阅第20章。  

## 总结
在本章中，我们首先阐述了典型GPU的工作原理及其与传统CPU的区别。我们说明了GPU如何通过牺牲加速单一指令流的处理器特性来换取更多处理单元，从而实现对海量数据的高度优化。  

我们阐述了GPU如何利用宽SIMD指令并行处理多个数据元素，以及如何通过谓词执行和掩码技术，在复杂流程控制的内核中实现SIMD指令的运行。我们探讨了谓词执行和掩码可能降低SIMD效率，导致高分歧性内核性能下降的现象，并分析了沿不同维度进行并行化选择对减少SIMD分歧的潜在影响。  

由于GPU拥有极其丰富的计算资源，我们阐述了保持较高占用率对充分发挥其性能的重要性。同时我们分析了GPU如何通过指令流机制来隐藏延迟，这使得为GPU提供充足的计算任务变得更为关键。  

接下来，我们讨论了将内核传输到GPU所涉及的软硬件层次结构及其开销成本，并分析了在单一设备上执行算法可能比在设备间转移执行更高效的情形。  

最后，我们阐述了内核在GPU上执行时的最佳实践。我们说明了为何许多内核最初受限于内存带宽，并详述了如何高效访问全局内存与局部内存，或通过子组操作彻底规避局部内存的使用。当内核转为计算瓶颈时，我们探讨了如何通过牺牲精度换取更高性能，或利用定制GPU扩展调用专用指令集来优化计算。  

### 如需了解更多信息  
关于GPU编程还有更多知识需要学习，本章内容仅触及皮毛！

GPU规格说明和白皮书是深入了解特定GPU及其架构的重要途径。许多GPU厂商会提供关于其产品的详尽信息及编程指导。

截至本书撰写时，关于主流GPU的相关资料可在software.intel.com、devblogs.nvidia.com和amd.com获取。

部分GPU厂商开源了驱动程序或驱动组件，通过研读或单步调试这些代码，开发者能直观认识到哪些操作存在较高开销，或发现应用程序中可能存在的性能瓶颈——这类实践极具教学价值。  

本章重点讨论了通过缓冲区访问器或统一共享内存（Unified Shared Memory）访问全局内存的传统方法，但大多数GPU还包含一个固定功能的纹理采样器，可加速图像处理操作。有关图像和采样器的更多信息，请参阅SYCL规范。  
